{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCEL (Langchain Expression Language)\n",
    "It is a concept within the LangChain framework, which is used \n",
    "for building applications that rely on LLMs. LCEL provides a \n",
    "way to express, manipulate, and reason about the flow of data \n",
    "and logic in these applications.\n",
    "\n",
    "For instance, in a LangChain application, you might use LCEL \n",
    "to define how a user's input should be processed, how to \n",
    "select the appropriate tools or agents to handle the task, \n",
    "and how to format the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Open AI API Key and Open SOurce Models--Llama3, Gemma2, Mistral\n",
    "## Uisng Groq API Key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Groq LPU\n",
    "The Groq Language Processing Unit, the LPU, is the technology that meets this moment. The LPU delivers instant speed, unparalleled affordability, and energy efficiency at scale. Fundamentally different from the GPU – originally designed for graphics processing – the LPU was designed for AI inference and language.\n",
    "\n",
    "Groq LPU AI inference technology is fundamentally different from the GPU, which was originally designed for graphics processing.​\n",
    "* Groq Compiler is in control, and not secondary to hardware.\n",
    "Compute and memory are co-located on the chip, eliminating resource bottlenecks.\n",
    "* Kernel-less compiler makes it easy and fast to compile new models.  \n",
    "* No caches and switches means seamless scalability.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "import openai\n",
    "openai.api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "groq_api_key = os.getenv('GROQ_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain_groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x000001FF4F8596F0>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x000001FF4F85B2E0>, model_name='gemma2-9b-it', groq_api_key=SecretStr('**********'))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_groq import ChatGroq\n",
    "\n",
    "model = ChatGroq(model='gemma2-9b-it', api_key=groq_api_key)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install langchain_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='¡Hola! ¿Cómo estás? \\n', response_metadata={'token_usage': {'completion_tokens': 11, 'prompt_tokens': 28, 'total_tokens': 39, 'completion_time': 0.020992965, 'prompt_time': 0.002698077, 'queue_time': None, 'total_time': 0.023691042}, 'model_name': 'gemma2-9b-it', 'system_fingerprint': 'fp_10c08bf97d', 'finish_reason': 'stop', 'logprobs': None}, id='run-11c912b7-60c8-46d8-8a62-8494e7a24e4c-0', usage_metadata={'input_tokens': 28, 'output_tokens': 11, 'total_tokens': 39})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage \n",
    "from langchain_core.messages import SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"Translate the following from English to Spanish Language\"),\n",
    "    HumanMessage(content=\"Hello, How are you?\")\n",
    "]\n",
    "\n",
    "model.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡Hola! ¿Cómo estás? \\n'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(messages).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hola, ¿cómo estás? \\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "response = model.invoke(messages)\n",
    "parser = StrOutputParser()\n",
    "parser.invoke(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"¡Hola! ¿Cómo estás? \\n\\n\\nLet me know if you have anything else you'd like me to translate!\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Using LCEL - We can chain the components\n",
    "\n",
    "chain = model | parser\n",
    "chain.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prompt Templates\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "generic_template = \"Translate the following into {language}\"\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", generic_template),\n",
    "    (\"user\", \"{text}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = prompt.invoke({\"language\":\"french\", \"text\":\"Hello\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content='Translate the following into french'),\n",
       " HumanMessage(content='Hello')]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.to_messages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bonjour! \\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain = prompt | model | parser\n",
    "chain.invoke({\"language\":\"French\", \"text\":\"Hello\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install fastapi uvicorn langserve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
