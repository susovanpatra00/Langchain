{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os                                                                       # For accessing environment variables\n",
    "from dotenv import load_dotenv                                                  # For loading environment variables from a .env file\n",
    "import bs4                                                                      # BeautifulSoup, for web scraping and parsing HTML/XML documents\n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader                  # Data Ingestion: Loading web content\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter             # Data Transformation: Splitting text into chunks\n",
    "from langchain.embeddings import OllamaEmbeddings                               # Embedding: Converting text into vector representations\n",
    "from langchain_chroma import Chroma                                             # Vector Store: Storing and retrieving vectors\n",
    "from langchain_core.prompts import ChatPromptTemplate                           # Prompt Template: Structuring prompts for the LLM\n",
    "from langchain_groq import ChatGroq                                             # LLM: The large language model for generating responses\n",
    "from langchain.chains import create_retrieval_chain                             # Retrieval Chain: Combining retriever and LLM for Q&A\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain     # Combining documents for processing\n",
    "from langchain_core.messages import AIMessage, HumanMessage                     # Message structures for conversation\n",
    "from langchain_community.chat_message_histories import ChatMessageHistory       # For maintaining chat histories\n",
    "from langchain_core.chat_history import BaseChatMessageHistory                  # Base class for chat history\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory         # For executing tasks with chat history\n",
    "from langchain.chains import create_history_aware_retriever                     # Creating retrievers that are aware of history\n",
    "from langchain_core.prompts import MessagesPlaceholder                          # Placeholder for messages in prompts\n",
    "\n",
    "load_dotenv()                                                                   # Load environment variables from the .env file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Ingestion\n",
    "#    Load, chunk and index the contents of the blog to create a retriever.\n",
    "\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Data Transformation\n",
    "#    We will convert the document into chunks for processing\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Embedding\n",
    "#    After converting the documents into chunks we need to convert those into vectors\n",
    "\n",
    "embeddings = OllamaEmbeddings(model=\"gemma2:2b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Vector Database \n",
    "#    We have created the ebeddings, now need to apply those embeddings and store it into database.\n",
    "\n",
    "vector_store = Chroma.from_documents(documents= chunks,embedding= embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VectorStoreRetriever(tags=['Chroma', 'OllamaEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x107881720>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Retrieval Chain\n",
    "\n",
    "retriever = vector_store.as_retriever()\n",
    "retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. LLM Initialisation\n",
    "\n",
    "groq_api_key=os.getenv(\"GROQ_API_KEY\")\n",
    "llm = ChatGroq(groq_api_key=groq_api_key,model_name=\"Llama3-8b-8192\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Contextualized Prompt\n",
    "\n",
    "contextualized_q_system_prompt = (\n",
    "    \"Given a chat history and the latest user question \"\n",
    "    \"which might reference context in the chat history, \"\n",
    "    \"formulate a standalone question which can be understood \"\n",
    "    \"without the chat history. Do NOT answer the question, \"\n",
    "    \"just reformulate it if needed and otherwise return it as is.\"\n",
    ")\n",
    "\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualized_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Question Answer Prompt with no History\n",
    "\n",
    "system_prompt = (\n",
    "    \"\"\"\n",
    "        You are an assistant for question-answering tasks.\n",
    "        Use the following pieces of retrieved context to answer\n",
    "        the question. If you don't know the answer, say that you \n",
    "        don't know. Use three sentences at maximum and keep the\n",
    "        answer concise.\n",
    "        \\n\\n\n",
    "        {context}\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBranch(branches=[(RunnableLambda(lambda x: not x.get('chat_history', False)), RunnableLambda(lambda x: x['input'])\n",
       "| VectorStoreRetriever(tags=['Chroma', 'OllamaEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x107881720>))], default=ChatPromptTemplate(input_variables=['chat_history', 'input'], input_types={'chat_history': typing.List[typing.Union[langchain_core.messages.ai.AIMessage, langchain_core.messages.human.HumanMessage, langchain_core.messages.chat.ChatMessage, langchain_core.messages.system.SystemMessage, langchain_core.messages.function.FunctionMessage, langchain_core.messages.tool.ToolMessage]]}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='Given a chat history and the latest user question which might reference context in the chat history, formulate a standalone question which can be understood without the chat history. Do NOT answer the question, just reformulate it if needed and otherwise return it as is.')), MessagesPlaceholder(variable_name='chat_history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], template='{input}'))])\n",
       "| ChatGroq(client=<groq.resources.chat.completions.Completions object at 0x147c63730>, async_client=<groq.resources.chat.completions.AsyncCompletions object at 0x147ca02e0>, model_name='Llama3-8b-8192', groq_api_key=SecretStr('**********'))\n",
       "| StrOutputParser()\n",
       "| VectorStoreRetriever(tags=['Chroma', 'OllamaEmbeddings'], vectorstore=<langchain_chroma.vectorstores.Chroma object at 0x107881720>)), config={'run_name': 'chat_retriever_chain'})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 9. History Aware Retriever Chain\n",
    "\n",
    "history_aware_retriever = create_history_aware_retriever(llm, retriever, contextualize_q_prompt)\n",
    "history_aware_retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Question Answer Chain\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm,qa_prompt)\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever,question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. The Full Conversational Chain With Session History\n",
    "\n",
    "store = {}\n",
    "\n",
    "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
    "    if session_id not in store:\n",
    "        store[session_id] = ChatMessageHistory()\n",
    "    return store[session_id]\n",
    "\n",
    "\n",
    "conversational_rag_chain = RunnableWithMessageHistory(\n",
    "    rag_chain,\n",
    "    get_session_history,\n",
    "    input_messages_key=\"input\",\n",
    "    history_messages_key=\"chat_history\",\n",
    "    output_messages_key=\"answer\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition is a process in which a complex task is broken down into smaller, more manageable subtasks.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is Task Decomposition?\"},\n",
    "    config={\n",
    "        \"configurable\": {\"session_id\": \"abc123\"}\n",
    "    },\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One common way of task decomposition is through the use of chain of thought (CoT) prompting, where a model is instructed to \"think step by step\" to decompose a hard task into smaller and simpler steps.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What are common ways of doing it?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"abc123\"}},\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"I don't know.\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A new session\n",
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is Maximum Inner Product Search (MIPS)?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"def123\"}},\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'According to the provided context, Chain of Thought (CoT) is a prompting technique used to enhance model performance on complex tasks. It instructs the model to \"think step by step\" to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and sheds light on the model\\'s thinking process.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# A new session\n",
    "conversational_rag_chain.invoke(\n",
    "    {\"input\": \"What is Chain of Thought?\"},\n",
    "    config={\"configurable\": {\"session_id\": \"ghi123\"}},\n",
    ")[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'abc123': InMemoryChatMessageHistory(messages=[HumanMessage(content='What is Task Decomposition?'), AIMessage(content='Task decomposition is a process in which a complex task is broken down into smaller, more manageable subtasks.'), HumanMessage(content='What are common ways of doing it?'), AIMessage(content='One common way of task decomposition is through the use of chain of thought (CoT) prompting, where a model is instructed to \"think step by step\" to decompose a hard task into smaller and simpler steps.')]),\n",
       " 'def123': InMemoryChatMessageHistory(messages=[HumanMessage(content='What is Maximum Inner Product Search (MIPS)?'), AIMessage(content=\"I don't know.\")]),\n",
       " 'ghi123': InMemoryChatMessageHistory(messages=[HumanMessage(content='What is Chain of Thought?'), AIMessage(content='According to the provided context, Chain of Thought (CoT) is a prompting technique used to enhance model performance on complex tasks. It instructs the model to \"think step by step\" to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and sheds light on the model\\'s thinking process.')])}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I made 3 different sessions and I can see it is being stored and passed everytime I use that session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
